{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17460b3-6aac-4ccb-9608-bfa151f75e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load genetic profiles data\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "\n",
    "# Load nutritional data\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051c539c-26ae-4fe3-a537-8fda151fc3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Profile_ID  Age   BMI   MC4R_Variant PPARG_Variant   FTO_Variant  \\\n",
      "0           1   51  24.5  rs17782313_TT  rs1801282_CC  rs9939609_TT   \n",
      "1           2   54  39.1  rs17782313_TT  rs1801282_CG  rs9939609_AA   \n",
      "2           3   43  29.4  rs17782313_CT  rs1801282_GG  rs9939609_AT   \n",
      "3           4   43  33.7  rs17782313_CT  rs1801282_CG  rs9939609_TT   \n",
      "4           5   26  21.3  rs17782313_TT  rs1801282_CG  rs9939609_AT   \n",
      "\n",
      "   LEPR_Variant  Physical_Activity  Diet_Type  Obesity_Risk_Score  \n",
      "0  rs1137101_AG                  0          2                0.42  \n",
      "1  rs1137101_AG                  0          1                0.40  \n",
      "2  rs1137101_AA                  0          0                0.93  \n",
      "3  rs1137101_AA                  0          2                0.80  \n",
      "4  rs1137101_AA                  0          2                0.28  \n",
      "Profile_ID            0\n",
      "Age                   0\n",
      "BMI                   0\n",
      "MC4R_Variant          0\n",
      "PPARG_Variant         0\n",
      "FTO_Variant           0\n",
      "LEPR_Variant          0\n",
      "Physical_Activity     0\n",
      "Diet_Type             0\n",
      "Obesity_Risk_Score    0\n",
      "dtype: int64\n",
      "      ID                       FoodGroup  \\\n",
      "0  16116     Legumes and Legume Products   \n",
      "1  18316                  Baked Products   \n",
      "2  15261  Finfish and Shellfish Products   \n",
      "3   8417               Breakfast Cereals   \n",
      "4  20022         Cereal Grains and Pasta   \n",
      "\n",
      "                                             Descrip  Energy_kcal  Protein_g  \\\n",
      "0                       Soy flour, full-fat, roasted        441.0      34.80   \n",
      "1        Pie, coconut custard, commercially prepared        260.0       5.90   \n",
      "2                                 Fish, tilapia, raw         96.0      20.08   \n",
      "3  Cereals, QUAKER, Instant Oatmeal, Banana Bread...        368.0       8.97   \n",
      "4               Cornmeal, degermed, enriched, yellow        370.0       7.11   \n",
      "\n",
      "   Fat_g  Carb_g  Sugar_g  Fiber_g  VitA_mcg  ...  Folate_USRDA  Niacin_USRDA  \\\n",
      "0  21.86   33.67     7.61      9.7       6.0  ...        0.5675      0.205375   \n",
      "1  13.20   30.20     0.00      1.8      26.0  ...        0.0475      0.025188   \n",
      "2   1.70    0.00     0.00      0.0       0.0  ...        0.0600      0.243938   \n",
      "3   4.85   75.70    29.45      6.7       0.0  ...        0.0000      0.706875   \n",
      "4   1.75   79.45     1.61      3.9      11.0  ...        0.8375      0.310500   \n",
      "\n",
      "   Riboflavin_USRDA  Thiamin_USRDA  Calcium_USRDA  Copper_USRDA  \\\n",
      "0          0.723846       0.343333       0.156667      0.002468   \n",
      "1          0.113846       0.073333       0.067500      0.000070   \n",
      "2          0.048462       0.034167       0.008333      0.000083   \n",
      "3          0.769231       0.816667       0.230833      0.000000   \n",
      "4          0.293846       0.459167       0.002500      0.000084   \n",
      "\n",
      "   Magnesium_USRDA  Phosphorus_USRDA  Selenium_USRDA  Zinc_USRDA  \n",
      "0         0.878571          0.680000        0.136364    0.325455  \n",
      "1         0.042857          0.174286        0.116364    0.061818  \n",
      "2         0.064286          0.242857        0.760000    0.030000  \n",
      "3         0.219048          0.450000        0.000000    0.188182  \n",
      "4         0.076190          0.141429        0.190909    0.060000  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "ID                  0\n",
      "FoodGroup           0\n",
      "Descrip             0\n",
      "Energy_kcal         0\n",
      "Protein_g           0\n",
      "Fat_g               0\n",
      "Carb_g              0\n",
      "Sugar_g             0\n",
      "Fiber_g             0\n",
      "VitA_mcg            0\n",
      "VitB6_mg            0\n",
      "VitB12_mcg          0\n",
      "VitC_mg             0\n",
      "VitE_mg             0\n",
      "Folate_mcg          0\n",
      "Niacin_mg           0\n",
      "Riboflavin_mg       0\n",
      "Thiamin_mg          0\n",
      "Calcium_mg          0\n",
      "Copper_mcg          0\n",
      "Iron_mg             0\n",
      "Magnesium_mg        0\n",
      "Manganese_mg        0\n",
      "Phosphorus_mg       0\n",
      "Selenium_mcg        0\n",
      "Zinc_mg             0\n",
      "VitA_USRDA          0\n",
      "VitB6_USRDA         0\n",
      "VitB12_USRDA        0\n",
      "VitC_USRDA          0\n",
      "VitE_USRDA          0\n",
      "Folate_USRDA        0\n",
      "Niacin_USRDA        0\n",
      "Riboflavin_USRDA    0\n",
      "Thiamin_USRDA       0\n",
      "Calcium_USRDA       0\n",
      "Copper_USRDA        0\n",
      "Magnesium_USRDA     0\n",
      "Phosphorus_USRDA    0\n",
      "Selenium_USRDA      0\n",
      "Zinc_USRDA          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check genetic profiles data\n",
    "print(genetic_df.head())\n",
    "print(genetic_df.isnull().sum())\n",
    "\n",
    "# Check nutritional data\n",
    "print(meal_df.head())\n",
    "print(meal_df.isnull().sum())\n",
    "\n",
    "# Fill missing values for numeric columns only\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcfec7c5-f5a2-4447-a927-d822d2bf45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for categorical columns\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80605623-0165-4ee6-92b6-68191156a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and target (y)\n",
    "X_genetic = genetic_df.drop(['Obesity_Risk_Score'], axis=1)\n",
    "y_genetic = genetic_df['Obesity_Risk_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9520ac-b068-4c0b-bfa9-6563a115ddc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile_ID            0\n",
      "Age                   0\n",
      "BMI                   0\n",
      "MC4R_Variant          0\n",
      "PPARG_Variant         0\n",
      "FTO_Variant           0\n",
      "LEPR_Variant          0\n",
      "Physical_Activity     0\n",
      "Diet_Type             0\n",
      "Obesity_Risk_Score    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(genetic_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1412bda0-b790-42d5-9c17-b15425a08909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical columns\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a460ce32-e8c3-41aa-b2a7-914af5937f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.07158341387500002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1036, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1548, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Obesity Risk Score: 0.4735000000000001\n",
      "Recommended Meal Cluster: 1\n",
      "                                                Descrip  Energy_kcal  \\\n",
      "0                          Soy flour, full-fat, roasted        441.0   \n",
      "15    Fast foods, english muffin, with cheese and sa...        338.0   \n",
      "19         Peanuts, virginia, oil-roasted, without salt        578.0   \n",
      "20              Cheesefurter, cheese smokie, pork, beef        328.0   \n",
      "26    Restaurant, family style, fried mozzarella sticks        325.0   \n",
      "...                                                 ...          ...   \n",
      "6869  BURGER KING, CROISSAN'WICH with Sausage and Ch...        376.0   \n",
      "6874  Pork, fresh, spareribs, separable lean and fat...        397.0   \n",
      "6877  Margarine-like, vegetable oil-butter spread, r...        450.0   \n",
      "6879  DOMINO'S 14\" Pepperoni Pizza, Classic Hand-Tos...        273.0   \n",
      "6885  Beef, Australian, Wagyu, rib, small end rib st...        330.0   \n",
      "\n",
      "      Protein_g  Fat_g  Carb_g  \n",
      "0         34.80  21.86   33.67  \n",
      "15        13.28  20.67   25.28  \n",
      "19        25.87  48.62   19.86  \n",
      "20        14.10  29.00    1.51  \n",
      "26        14.75  18.33   25.14  \n",
      "...         ...    ...     ...  \n",
      "6869      13.73  25.45   23.00  \n",
      "6874      29.06  30.30    0.00  \n",
      "6877       1.00  50.00    1.00  \n",
      "6879      11.25  11.18   31.86  \n",
      "6885      17.61  28.61    0.46  \n",
      "\n",
      "[779 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Encode categorical columns in genetic_df\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "\n",
    "# Prepare features and target for genetic data\n",
    "X_genetic = genetic_df.drop(['Obesity_Risk_Score'], axis=1)\n",
    "y_genetic = genetic_df['Obesity_Risk_Score']\n",
    "\n",
    "# Split genetic data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_genetic, y_genetic, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Preprocess nutritional data\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a mapping function to map risk scores to meal clusters\n",
    "def map_risk_to_cluster(risk_score):\n",
    "    if risk_score < 0.3:\n",
    "        return 0  # Low-risk cluster\n",
    "    elif 0.3 <= risk_score < 0.6:\n",
    "        return 1  # Medium-risk cluster\n",
    "    else:\n",
    "        return 2  # High-risk cluster\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = pd.DataFrame({\n",
    "    'BMI': [24.5],\n",
    "    'MC4R_Variant': ['rs17782313_TT'],\n",
    "    'PPARG_Variant': ['rs1801282_CC'],\n",
    "    'FTO_Variant': ['rs9939609_TT'],\n",
    "    'LEPR_Variant': ['rs1137101_AG'],\n",
    "    'Physical_Activity': ['High'],\n",
    "    'Diet_Type': ['High-Fat']\n",
    "})\n",
    "\n",
    "# Encode categorical variables in the new profile\n",
    "new_profile = pd.get_dummies(new_profile, drop_first=True)\n",
    "\n",
    "# Ensure the new profile has the same columns as the training data\n",
    "missing_cols = set(X_train.columns) - set(new_profile.columns)\n",
    "for col in missing_cols:\n",
    "    new_profile[col] = 0\n",
    "new_profile = new_profile[X_train.columns]\n",
    "\n",
    "# Predict risk score for the new profile\n",
    "predicted_risk = rf.predict(new_profile)\n",
    "print(f'Predicted Obesity Risk Score: {predicted_risk[0]}')\n",
    "\n",
    "# Map predicted risk score to a meal cluster\n",
    "recommended_cluster = map_risk_to_cluster(predicted_risk[0])\n",
    "print(f'Recommended Meal Cluster: {recommended_cluster}')\n",
    "\n",
    "# Recommend meals from the recommended cluster\n",
    "recommended_meals = meal_df[meal_df['Meal_Cluster'] == recommended_cluster]\n",
    "print(recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ca3bb67-46f2-440d-bde5-4bd4deb3b989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Accuracy: 0.4175\n",
      "Precision: 0.35652521929824565\n",
      "Recall: 0.4175\n",
      "F1 Score: 0.37008044889766223\n",
      "Predicted Obesity Risk Category: Medium\n",
      "Recommended Meal Cluster: 1\n",
      "                                                Descrip  Energy_kcal  \\\n",
      "0                          Soy flour, full-fat, roasted        441.0   \n",
      "15    Fast foods, english muffin, with cheese and sa...        338.0   \n",
      "19         Peanuts, virginia, oil-roasted, without salt        578.0   \n",
      "20              Cheesefurter, cheese smokie, pork, beef        328.0   \n",
      "26    Restaurant, family style, fried mozzarella sticks        325.0   \n",
      "...                                                 ...          ...   \n",
      "6869  BURGER KING, CROISSAN'WICH with Sausage and Ch...        376.0   \n",
      "6874  Pork, fresh, spareribs, separable lean and fat...        397.0   \n",
      "6877  Margarine-like, vegetable oil-butter spread, r...        450.0   \n",
      "6879  DOMINO'S 14\" Pepperoni Pizza, Classic Hand-Tos...        273.0   \n",
      "6885  Beef, Australian, Wagyu, rib, small end rib st...        330.0   \n",
      "\n",
      "      Protein_g  Fat_g  Carb_g  \n",
      "0         34.80  21.86   33.67  \n",
      "15        13.28  20.67   25.28  \n",
      "19        25.87  48.62   19.86  \n",
      "20        14.10  29.00    1.51  \n",
      "26        14.75  18.33   25.14  \n",
      "...         ...    ...     ...  \n",
      "6869      13.73  25.45   23.00  \n",
      "6874      29.06  30.30    0.00  \n",
      "6877       1.00  50.00    1.00  \n",
      "6879      11.25  11.18   31.86  \n",
      "6885      17.61  28.61    0.46  \n",
      "\n",
      "[779 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Encode categorical columns in genetic_df\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Prepare features and target for genetic data\n",
    "X_genetic = genetic_df.drop(['Obesity_Risk_Score', 'Obesity_Risk_Category'], axis=1)\n",
    "y_genetic = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Split genetic data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_genetic, y_genetic, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Preprocess nutritional data\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a mapping function to map risk categories to meal clusters\n",
    "def map_risk_to_cluster(risk_category):\n",
    "    if risk_category == 'Low':\n",
    "        return 0  # Low-risk cluster\n",
    "    elif risk_category == 'Medium':\n",
    "        return 1  # Medium-risk cluster\n",
    "    else:\n",
    "        return 2  # High-risk cluster\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = pd.DataFrame({\n",
    "    'BMI': [24.5],\n",
    "    'MC4R_Variant': ['rs17782313_TT'],\n",
    "    'PPARG_Variant': ['rs1801282_CC'],\n",
    "    'FTO_Variant': ['rs9939609_TT'],\n",
    "    'LEPR_Variant': ['rs1137101_AG'],\n",
    "    'Physical_Activity': ['High'],\n",
    "    'Diet_Type': ['High-Fat']\n",
    "})\n",
    "\n",
    "# Encode categorical variables in the new profile\n",
    "new_profile = pd.get_dummies(new_profile, drop_first=True)\n",
    "\n",
    "# Ensure the new profile has the same columns as the training data\n",
    "missing_cols = set(X_train.columns) - set(new_profile.columns)\n",
    "for col in missing_cols:\n",
    "    new_profile[col] = 0\n",
    "new_profile = new_profile[X_train.columns]\n",
    "\n",
    "# Predict risk category for the new profile\n",
    "predicted_risk_category = best_rf.predict(new_profile)\n",
    "print(f'Predicted Obesity Risk Category: {predicted_risk_category[0]}')\n",
    "\n",
    "# Map predicted risk category to a meal cluster\n",
    "recommended_cluster = map_risk_to_cluster(predicted_risk_category[0])\n",
    "print(f'Recommended Meal Cluster: {recommended_cluster}')\n",
    "\n",
    "# Recommend meals from the recommended cluster\n",
    "recommended_meals = meal_df[meal_df['Meal_Cluster'] == recommended_cluster]\n",
    "print(recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "504ff12e-08bc-4bf9-a91f-23be49c510f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obesity_Risk_Category\n",
      "High      848\n",
      "Medium    696\n",
      "Low       456\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_genetic.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e231b5-6699-4d83-a929-d12b1f76ef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Accuracy: 48.52652259332024\n",
      "Precision: 48.332260336442836\n",
      "Recall: 48.52652259332024\n",
      "F1 Score: 48.38473848257559\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.50      0.47      0.48       184\n",
      "         Low       0.56      0.60      0.58       161\n",
      "      Medium       0.40      0.39      0.39       164\n",
      "\n",
      "    accuracy                           0.49       509\n",
      "   macro avg       0.48      0.49      0.48       509\n",
      "weighted avg       0.48      0.49      0.48       509\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[86 34 64]\n",
      " [30 97 34]\n",
      " [57 43 64]]\n",
      "Predicted Obesity Risk Category: Medium\n",
      "Recommended Meal Cluster: 1\n",
      "                                                Descrip  Energy_kcal  \\\n",
      "0                          Soy flour, full-fat, roasted        441.0   \n",
      "15    Fast foods, english muffin, with cheese and sa...        338.0   \n",
      "19         Peanuts, virginia, oil-roasted, without salt        578.0   \n",
      "20              Cheesefurter, cheese smokie, pork, beef        328.0   \n",
      "26    Restaurant, family style, fried mozzarella sticks        325.0   \n",
      "...                                                 ...          ...   \n",
      "6869  BURGER KING, CROISSAN'WICH with Sausage and Ch...        376.0   \n",
      "6874  Pork, fresh, spareribs, separable lean and fat...        397.0   \n",
      "6877  Margarine-like, vegetable oil-butter spread, r...        450.0   \n",
      "6879  DOMINO'S 14\" Pepperoni Pizza, Classic Hand-Tos...        273.0   \n",
      "6885  Beef, Australian, Wagyu, rib, small end rib st...        330.0   \n",
      "\n",
      "      Protein_g  Fat_g  Carb_g  \n",
      "0         34.80  21.86   33.67  \n",
      "15        13.28  20.67   25.28  \n",
      "19        25.87  48.62   19.86  \n",
      "20        14.10  29.00    1.51  \n",
      "26        14.75  18.33   25.14  \n",
      "...         ...    ...     ...  \n",
      "6869      13.73  25.45   23.00  \n",
      "6874      29.06  30.30    0.00  \n",
      "6877       1.00  50.00    1.00  \n",
      "6879      11.25  11.18   31.86  \n",
      "6885      17.61  28.61    0.46  \n",
      "\n",
      "[779 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE  # Import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Encode categorical columns in genetic_df\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Prepare features and target for genetic data\n",
    "X_genetic = genetic_df.drop(['Obesity_Risk_Score', 'Obesity_Risk_Category'], axis=1)\n",
    "y_genetic = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_genetic, y_genetic)\n",
    "\n",
    "# Split resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "precision = precision_score(y_test, y_pred, average='weighted') \n",
    "recall = recall_score(y_test, y_pred, average='weighted') \n",
    "f1 = f1_score(y_test, y_pred, average='weighted') \n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Preprocess nutritional data\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a mapping function to map risk categories to meal clusters\n",
    "def map_risk_to_cluster(risk_category):\n",
    "    if risk_category == 'Low':\n",
    "        return 0  # Low-risk cluster\n",
    "    elif risk_category == 'Medium':\n",
    "        return 1  # Medium-risk cluster\n",
    "    else:\n",
    "        return 2  # High-risk cluster\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = pd.DataFrame({\n",
    "    'BMI': [24.5],\n",
    "    'MC4R_Variant': ['rs17782313_TT'],\n",
    "    'PPARG_Variant': ['rs1801282_CC'],\n",
    "    'FTO_Variant': ['rs9939609_TT'],\n",
    "    'LEPR_Variant': ['rs1137101_AG'],\n",
    "    'Physical_Activity': ['High'],\n",
    "    'Diet_Type': ['High-Fat']\n",
    "})\n",
    "\n",
    "# Encode categorical variables in the new profile\n",
    "new_profile = pd.get_dummies(new_profile, drop_first=True)\n",
    "\n",
    "# Ensure the new profile has the same columns as the training data\n",
    "missing_cols = set(X_train.columns) - set(new_profile.columns)\n",
    "for col in missing_cols:\n",
    "    new_profile[col] = 0\n",
    "new_profile = new_profile[X_train.columns]\n",
    "\n",
    "# Predict risk category for the new profile\n",
    "predicted_risk_category = best_rf.predict(new_profile)\n",
    "print(f'Predicted Obesity Risk Category: {predicted_risk_category[0]}')\n",
    "\n",
    "# Map predicted risk category to a meal cluster\n",
    "recommended_cluster = map_risk_to_cluster(predicted_risk_category[0])\n",
    "print(f'Recommended Meal Cluster: {recommended_cluster}')\n",
    "\n",
    "# Recommend meals from the recommended cluster\n",
    "recommended_meals = meal_df[meal_df['Meal_Cluster'] == recommended_cluster]\n",
    "print(recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8c89146-4f65-463a-9682-f7f87dcbc9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  Importance\n",
      "0         Profile_ID    0.243472\n",
      "2                BMI    0.224138\n",
      "1                Age    0.203519\n",
      "8          Diet_Type    0.057196\n",
      "5        FTO_Variant    0.056020\n",
      "3       MC4R_Variant    0.055008\n",
      "4      PPARG_Variant    0.054802\n",
      "6       LEPR_Variant    0.053823\n",
      "7  Physical_Activity    0.052023\n"
     ]
    }
   ],
   "source": [
    "importances = best_rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60d1f686-1a8c-40cb-bc2c-ea52a30bd19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Accuracy: 0.4911591355599214\n",
      "Precision: 0.4885500967102819\n",
      "Recall: 0.4911591355599214\n",
      "F1 Score: 0.48830630696733346\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.51      0.45      0.48       184\n",
      "         Low       0.55      0.63      0.59       161\n",
      "      Medium       0.40      0.40      0.40       164\n",
      "\n",
      "    accuracy                           0.49       509\n",
      "   macro avg       0.49      0.49      0.49       509\n",
      "weighted avg       0.49      0.49      0.49       509\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 83  43  58]\n",
      " [ 21 102  38]\n",
      " [ 59  40  65]]\n",
      "Predicted Obesity Risk Category: Medium\n",
      "Recommended Meal Cluster: 1\n",
      "                                                Descrip  Energy_kcal  \\\n",
      "0                          Soy flour, full-fat, roasted        441.0   \n",
      "15    Fast foods, english muffin, with cheese and sa...        338.0   \n",
      "19         Peanuts, virginia, oil-roasted, without salt        578.0   \n",
      "20              Cheesefurter, cheese smokie, pork, beef        328.0   \n",
      "26    Restaurant, family style, fried mozzarella sticks        325.0   \n",
      "...                                                 ...          ...   \n",
      "6869  BURGER KING, CROISSAN'WICH with Sausage and Ch...        376.0   \n",
      "6874  Pork, fresh, spareribs, separable lean and fat...        397.0   \n",
      "6877  Margarine-like, vegetable oil-butter spread, r...        450.0   \n",
      "6879  DOMINO'S 14\" Pepperoni Pizza, Classic Hand-Tos...        273.0   \n",
      "6885  Beef, Australian, Wagyu, rib, small end rib st...        330.0   \n",
      "\n",
      "      Protein_g  Fat_g  Carb_g  \n",
      "0         34.80  21.86   33.67  \n",
      "15        13.28  20.67   25.28  \n",
      "19        25.87  48.62   19.86  \n",
      "20        14.10  29.00    1.51  \n",
      "26        14.75  18.33   25.14  \n",
      "...         ...    ...     ...  \n",
      "6869      13.73  25.45   23.00  \n",
      "6874      29.06  30.30    0.00  \n",
      "6877       1.00  50.00    1.00  \n",
      "6879      11.25  11.18   31.86  \n",
      "6885      17.61  28.61    0.46  \n",
      "\n",
      "[779 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Drop irrelevant features (Profile_ID, Age, Physical_Activity)\n",
    "genetic_df = genetic_df.drop(['Profile_ID', 'Age', 'Physical_Activity'], axis=1)\n",
    "\n",
    "# One-hot encode categorical features (Diet_Type and genetic variants)\n",
    "genetic_df = pd.get_dummies(genetic_df, columns=['Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant'], drop_first=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X_genetic = genetic_df.drop(['Obesity_Risk_Score', 'Obesity_Risk_Category'], axis=1)\n",
    "y_genetic = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_genetic, y_genetic)\n",
    "\n",
    "# Split resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Preprocess nutritional data\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a mapping function to map risk categories to meal clusters\n",
    "def map_risk_to_cluster(risk_category):\n",
    "    if risk_category == 'Low':\n",
    "        return 0  # Low-risk cluster\n",
    "    elif risk_category == 'Medium':\n",
    "        return 1  # Medium-risk cluster\n",
    "    else:\n",
    "        return 2  # High-risk cluster\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = pd.DataFrame({\n",
    "    'BMI': [24.5],\n",
    "    'MC4R_Variant_rs17782313_CT': [0],\n",
    "    'MC4R_Variant_rs17782313_TT': [1],\n",
    "    'PPARG_Variant_rs1801282_CG': [0],\n",
    "    'PPARG_Variant_rs1801282_GG': [0],\n",
    "    'FTO_Variant_rs9939609_AT': [0],\n",
    "    'FTO_Variant_rs9939609_TT': [1],\n",
    "    'LEPR_Variant_rs1137101_AG': [1],\n",
    "    'LEPR_Variant_rs1137101_GG': [0],\n",
    "    'Diet_Type_High-Carb': [0],\n",
    "    'Diet_Type_High-Fat': [1]\n",
    "})\n",
    "\n",
    "# Ensure the new profile has the same columns as the training data\n",
    "missing_cols = set(X_train.columns) - set(new_profile.columns)\n",
    "for col in missing_cols:\n",
    "    new_profile[col] = 0\n",
    "new_profile = new_profile[X_train.columns]\n",
    "\n",
    "# Predict risk category for the new profile\n",
    "predicted_risk_category = best_rf.predict(new_profile)\n",
    "print(f'Predicted Obesity Risk Category: {predicted_risk_category[0]}')\n",
    "\n",
    "# Map predicted risk category to a meal cluster\n",
    "recommended_cluster = map_risk_to_cluster(predicted_risk_category[0])\n",
    "print(f'Recommended Meal Cluster: {recommended_cluster}')\n",
    "\n",
    "# Recommend meals from the recommended cluster\n",
    "recommended_meals = meal_df[meal_df['Meal_Cluster'] == recommended_cluster]\n",
    "print(recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be916489-17ec-4ece-a9db-5ffa5de0ee0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'High'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Apply SMOTE to balance the dataset\u001b[39;00m\n\u001b[0;32m     40\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Split resampled data into training and testing sets\u001b[39;00m\n\u001b[0;32m     44\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_resampled, y_resampled, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\base.py:99\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m     97\u001b[0m check_classification_targets(y)\n\u001b[0;32m     98\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m---> 99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\base.py:157\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    155\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    156\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 157\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:973\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[0;32m    972\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[1;32m--> 973\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[0;32m    975\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6639\u001b[0m     ]\n\u001b[0;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'High'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Select relevant features\n",
    "features = ['BMI', 'Physical_Activity', 'Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant', 'Obesity_Risk_Score']\n",
    "genetic_df = genetic_df[features + ['Obesity_Risk_Category']]\n",
    "\n",
    "# One-hot encode categorical features (Diet_Type and genetic variants)\n",
    "genetic_df = pd.get_dummies(genetic_df, columns=['Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant'], drop_first=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X = genetic_df.drop(['Obesity_Risk_Category'], axis=1)\n",
    "y = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='f1_weighted')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Preprocess nutritional data for meal recommendations\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a mapping function to map risk categories to meal clusters\n",
    "def map_risk_to_cluster(risk_category):\n",
    "    if risk_category == 'Low':\n",
    "        return 0  # Low-risk cluster\n",
    "    elif risk_category == 'Medium':\n",
    "        return 1  # Medium-risk cluster\n",
    "    else:\n",
    "        return 2  # High-risk cluster\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = pd.DataFrame({\n",
    "    'BMI': [24.5],\n",
    "    'Physical_Activity': [3],  # Assuming 3 is moderate activity\n",
    "    'Diet_Type_High-Carb': [0],\n",
    "    'Diet_Type_High-Fat': [1],\n",
    "    'MC4R_Variant_rs17782313_CT': [0],\n",
    "    'MC4R_Variant_rs17782313_TT': [1],\n",
    "    'PPARG_Variant_rs1801282_CG': [0],\n",
    "    'PPARG_Variant_rs1801282_GG': [0],\n",
    "    'FTO_Variant_rs9939609_AT': [0],\n",
    "    'FTO_Variant_rs9939609_TT': [1],\n",
    "    'LEPR_Variant_rs1137101_AG': [1],\n",
    "    'LEPR_Variant_rs1137101_GG': [0],\n",
    "    'Obesity_Risk_Score': [0.55]\n",
    "})\n",
    "\n",
    "# Ensure the new profile has the same columns as the training data\n",
    "missing_cols = set(X_train.columns) - set(new_profile.columns)\n",
    "for col in missing_cols:\n",
    "    new_profile[col] = 0\n",
    "new_profile = new_profile[X_train.columns]\n",
    "\n",
    "# Predict risk category for the new profile\n",
    "predicted_risk_category = best_rf.predict(new_profile)\n",
    "print(f'Predicted Obesity Risk Category: {predicted_risk_category[0]}')\n",
    "\n",
    "# Map predicted risk category to a meal cluster\n",
    "recommended_cluster = map_risk_to_cluster(predicted_risk_category[0])\n",
    "print(f'Recommended Meal Cluster: {recommended_cluster}')\n",
    "\n",
    "# Recommend meals from the recommended cluster\n",
    "recommended_meals = meal_df[meal_df['Meal_Cluster'] == recommended_cluster]\n",
    "print(recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70c6d701-3ab8-4654-9796-61155fca78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1. nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      1.00      1.00       184\n",
      "         Low       1.00      1.00      1.00       161\n",
      "      Medium       1.00      1.00      1.00       164\n",
      "\n",
      "    accuracy                           1.00       509\n",
      "   macro avg       1.00      1.00      1.00       509\n",
      "weighted avg       1.00      1.00      1.00       509\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[184   0   0]\n",
      " [  0 161   0]\n",
      " [  0   0 164]]\n",
      "Predicted Obesity Risk Category: Medium\n",
      "Recommended Meal Cluster: 1\n",
      "                                                Descrip  Energy_kcal  \\\n",
      "0                          Soy flour, full-fat, roasted        441.0   \n",
      "15    Fast foods, english muffin, with cheese and sa...        338.0   \n",
      "19         Peanuts, virginia, oil-roasted, without salt        578.0   \n",
      "20              Cheesefurter, cheese smokie, pork, beef        328.0   \n",
      "26    Restaurant, family style, fried mozzarella sticks        325.0   \n",
      "...                                                 ...          ...   \n",
      "6869  BURGER KING, CROISSAN'WICH with Sausage and Ch...        376.0   \n",
      "6874  Pork, fresh, spareribs, separable lean and fat...        397.0   \n",
      "6877  Margarine-like, vegetable oil-butter spread, r...        450.0   \n",
      "6879  DOMINO'S 14\" Pepperoni Pizza, Classic Hand-Tos...        273.0   \n",
      "6885  Beef, Australian, Wagyu, rib, small end rib st...        330.0   \n",
      "\n",
      "      Protein_g  Fat_g  Carb_g  \n",
      "0         34.80  21.86   33.67  \n",
      "15        13.28  20.67   25.28  \n",
      "19        25.87  48.62   19.86  \n",
      "20        14.10  29.00    1.51  \n",
      "26        14.75  18.33   25.14  \n",
      "...         ...    ...     ...  \n",
      "6869      13.73  25.45   23.00  \n",
      "6874      29.06  30.30    0.00  \n",
      "6877       1.00  50.00    1.00  \n",
      "6879      11.25  11.18   31.86  \n",
      "6885      17.61  28.61    0.46  \n",
      "\n",
      "[779 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Select relevant features\n",
    "features = ['BMI', 'Physical_Activity', 'Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant', 'Obesity_Risk_Score']\n",
    "genetic_df = genetic_df[features + ['Obesity_Risk_Category']]\n",
    "\n",
    "# One-hot encode categorical features (Diet_Type and genetic variants)\n",
    "genetic_df = pd.get_dummies(genetic_df, columns=['Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant'], drop_first=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X = genetic_df.drop(['Obesity_Risk_Category'], axis=1)\n",
    "y = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Convert Obesity_Risk_Category to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Ensure all columns in X are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "        X[col] = X[col].fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y_encoded)\n",
    "\n",
    "# Split resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test back to original labels for evaluation\n",
    "y_train_labels = label_encoder.inverse_transform(y_train)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='f1_weighted')\n",
    "grid_search.fit(X_train, y_train_labels)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "precision = precision_score(y_test_labels, y_pred, average='weighted')\n",
    "recall = recall_score(y_test_labels, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test_labels, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_labels, y_pred))\n",
    "\n",
    "# Preprocess nutritional data for meal recommendations\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a mapping function to map risk categories to meal clusters\n",
    "def map_risk_to_cluster(risk_category):\n",
    "    if risk_category == 'Low':\n",
    "        return 0  # Low-risk cluster\n",
    "    elif risk_category == 'Medium':\n",
    "        return 1  # Medium-risk cluster\n",
    "    else:\n",
    "        return 2  # High-risk cluster\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = pd.DataFrame({\n",
    "    'BMI': [24.5],\n",
    "    'Physical_Activity': [3],  # Assuming 3 is moderate activity\n",
    "    'Diet_Type_High-Carb': [0],\n",
    "    'Diet_Type_High-Fat': [1],\n",
    "    'MC4R_Variant_rs17782313_CT': [0],\n",
    "    'MC4R_Variant_rs17782313_TT': [1],\n",
    "    'PPARG_Variant_rs1801282_CG': [0],\n",
    "    'PPARG_Variant_rs1801282_GG': [0],\n",
    "    'FTO_Variant_rs9939609_AT': [0],\n",
    "    'FTO_Variant_rs9939609_TT': [1],\n",
    "    'LEPR_Variant_rs1137101_AG': [1],\n",
    "    'LEPR_Variant_rs1137101_GG': [0],\n",
    "    'Obesity_Risk_Score': [0.55]\n",
    "})\n",
    "\n",
    "# Ensure the new profile has the same columns as the training data\n",
    "missing_cols = set(X_train.columns) - set(new_profile.columns)\n",
    "for col in missing_cols:\n",
    "    new_profile[col] = 0\n",
    "new_profile = new_profile[X_train.columns]\n",
    "\n",
    "# Predict risk category for the new profile\n",
    "predicted_risk_category = best_rf.predict(new_profile)\n",
    "print(f'Predicted Obesity Risk Category: {predicted_risk_category[0]}')\n",
    "\n",
    "# Map predicted risk category to a meal cluster\n",
    "recommended_cluster = map_risk_to_cluster(predicted_risk_category[0])\n",
    "print(f'Recommended Meal Cluster: {recommended_cluster}')\n",
    "\n",
    "# Recommend meals from the recommended cluster\n",
    "recommended_meals = meal_df[meal_df['Meal_Cluster'] == recommended_cluster]\n",
    "print(recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30e74650-3884-4750-8309-e95c5937f35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      1.00      1.00       184\n",
      "         Low       1.00      1.00      1.00       161\n",
      "      Medium       1.00      1.00      1.00       164\n",
      "\n",
      "    accuracy                           1.00       509\n",
      "   macro avg       1.00      1.00      1.00       509\n",
      "weighted avg       1.00      1.00      1.00       509\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[184   0   0]\n",
      " [  0 161   0]\n",
      " [  0   0 164]]\n",
      "Predicted Obesity Risk Category: Medium\n",
      "Recommended Meal Cluster: 1\n",
      "                                                Descrip  Energy_kcal  \\\n",
      "0                          Soy flour, full-fat, roasted        441.0   \n",
      "15    Fast foods, english muffin, with cheese and sa...        338.0   \n",
      "19         Peanuts, virginia, oil-roasted, without salt        578.0   \n",
      "20              Cheesefurter, cheese smokie, pork, beef        328.0   \n",
      "26    Restaurant, family style, fried mozzarella sticks        325.0   \n",
      "...                                                 ...          ...   \n",
      "6869  BURGER KING, CROISSAN'WICH with Sausage and Ch...        376.0   \n",
      "6874  Pork, fresh, spareribs, separable lean and fat...        397.0   \n",
      "6877  Margarine-like, vegetable oil-butter spread, r...        450.0   \n",
      "6879  DOMINO'S 14\" Pepperoni Pizza, Classic Hand-Tos...        273.0   \n",
      "6885  Beef, Australian, Wagyu, rib, small end rib st...        330.0   \n",
      "\n",
      "      Protein_g  Fat_g  Carb_g  \n",
      "0         34.80  21.86   33.67  \n",
      "15        13.28  20.67   25.28  \n",
      "19        25.87  48.62   19.86  \n",
      "20        14.10  29.00    1.51  \n",
      "26        14.75  18.33   25.14  \n",
      "...         ...    ...     ...  \n",
      "6869      13.73  25.45   23.00  \n",
      "6874      29.06  30.30    0.00  \n",
      "6877       1.00  50.00    1.00  \n",
      "6879      11.25  11.18   31.86  \n",
      "6885      17.61  28.61    0.46  \n",
      "\n",
      "[779 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Select relevant features\n",
    "features = ['BMI', 'Physical_Activity', 'Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant', 'Obesity_Risk_Score']\n",
    "genetic_df = genetic_df[features + ['Obesity_Risk_Category']]\n",
    "\n",
    "# One-hot encode categorical features (Diet_Type and genetic variants)\n",
    "genetic_df = pd.get_dummies(genetic_df, columns=['Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant'], drop_first=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X = genetic_df.drop(['Obesity_Risk_Category'], axis=1)\n",
    "y = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Convert Obesity_Risk_Category to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Ensure all columns in X are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "        X[col] = X[col].fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y_encoded)\n",
    "\n",
    "# Split resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test back to original labels for evaluation\n",
    "y_train_labels = label_encoder.inverse_transform(y_train)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Updated Hyperparameter Grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']  # Removed 'auto'\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='f1_weighted')\n",
    "grid_search.fit(X_train, y_train_labels)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "precision = precision_score(y_test_labels, y_pred, average='weighted')\n",
    "recall = recall_score(y_test_labels, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test_labels, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_labels, y_pred))\n",
    "\n",
    "# Preprocess nutritional data for meal recommendations\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a mapping function to map risk categories to meal clusters\n",
    "def map_risk_to_cluster(risk_category):\n",
    "    if risk_category == 'Low':\n",
    "        return 0  # Low-risk cluster\n",
    "    elif risk_category == 'Medium':\n",
    "        return 1  # Medium-risk cluster\n",
    "    else:\n",
    "        return 2  # High-risk cluster\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = pd.DataFrame({\n",
    "    'BMI': [24.5],\n",
    "    'Physical_Activity': [3],  # Assuming 3 is moderate activity\n",
    "    'Diet_Type_High-Carb': [0],\n",
    "    'Diet_Type_High-Fat': [1],\n",
    "    'MC4R_Variant_rs17782313_CT': [0],\n",
    "    'MC4R_Variant_rs17782313_TT': [1],\n",
    "    'PPARG_Variant_rs1801282_CG': [0],\n",
    "    'PPARG_Variant_rs1801282_GG': [0],\n",
    "    'FTO_Variant_rs9939609_AT': [0],\n",
    "    'FTO_Variant_rs9939609_TT': [1],\n",
    "    'LEPR_Variant_rs1137101_AG': [1],\n",
    "    'LEPR_Variant_rs1137101_GG': [0],\n",
    "    'Obesity_Risk_Score': [0.55]\n",
    "})\n",
    "\n",
    "# Ensure the new profile has the same columns as the training data\n",
    "missing_cols = set(X_train.columns) - set(new_profile.columns)\n",
    "for col in missing_cols:\n",
    "    new_profile[col] = 0\n",
    "new_profile = new_profile[X_train.columns]\n",
    "\n",
    "# Predict risk category for the new profile\n",
    "predicted_risk_category = best_rf.predict(new_profile)\n",
    "print(f'Predicted Obesity Risk Category: {predicted_risk_category[0]}')\n",
    "\n",
    "# Map predicted risk category to a meal cluster\n",
    "recommended_cluster = map_risk_to_cluster(predicted_risk_category[0])\n",
    "print(f'Recommended Meal Cluster: {recommended_cluster}')\n",
    "\n",
    "# Recommend meals from the recommended cluster\n",
    "recommended_meals = meal_df[meal_df['Meal_Cluster'] == recommended_cluster]\n",
    "print(recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9229df9f-aca8-43b6-bab6-033f5bc10a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "\n",
      "Classification Report (Percentage):\n",
      "Class      Precision  Recall     F1 Score  \n",
      "High       100.00    % 100.00    % 100.00    %\n",
      "Low        100.00    % 100.00    % 100.00    %\n",
      "Medium     100.00    % 100.00    % 100.00    %\n",
      "\n",
      "Confusion Matrix:\n",
      "[[184   0   0]\n",
      " [  0 161   0]\n",
      " [  0   0 164]]\n",
      "Predicted Obesity Risk Category: Medium\n",
      "\n",
      "=== Recommended Meals for Medium-Risk Cluster ===\n",
      "Total Meals in Cluster: 779\n",
      "\n",
      "Top 5 Meals:\n",
      "Meal Description                                   Energy (kcal)   Protein (g)     Fat (g)         Carbs (g)      \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Soy flour, full-fat, roasted                       441.0           34.8            21.86           33.67          \n",
      "Fast foods, english muffin, with cheese and sausage 338.0           13.28           20.67           25.28          \n",
      "Peanuts, virginia, oil-roasted, without salt       578.0           25.87           48.62           19.86          \n",
      "Cheesefurter, cheese smokie, pork, beef            328.0           14.1            29.0            1.51           \n",
      "Restaurant, family style, fried mozzarella sticks  325.0           14.75           18.33           25.14          \n",
      "\n",
      "=== Nutritional Summary ===\n",
      "Average Energy (kcal): 347.87\n",
      "Average Protein (g): 16.66\n",
      "Average Fat (g): 26.54\n",
      "Average Carbs (g): 11.62\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Select relevant features\n",
    "features = ['BMI', 'Physical_Activity', 'Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant', 'Obesity_Risk_Score']\n",
    "genetic_df = genetic_df[features + ['Obesity_Risk_Category']]\n",
    "\n",
    "# One-hot encode categorical features (Diet_Type and genetic variants)\n",
    "genetic_df = pd.get_dummies(genetic_df, columns=['Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant'], drop_first=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X = genetic_df.drop(['Obesity_Risk_Category'], axis=1)\n",
    "y = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Convert Obesity_Risk_Category to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Ensure all columns in X are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "        X[col] = X[col].fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y_encoded)\n",
    "\n",
    "# Split resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test back to original labels for evaluation\n",
    "y_train_labels = label_encoder.inverse_transform(y_train)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Updated Hyperparameter Grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']  # Removed 'auto'\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='f1_weighted')\n",
    "grid_search.fit(X_train, y_train_labels)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics as percentages\n",
    "accuracy = accuracy_score(y_test_labels, y_pred) * 100\n",
    "precision = precision_score(y_test_labels, y_pred, average='weighted') * 100\n",
    "recall = recall_score(y_test_labels, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test_labels, y_pred, average='weighted') * 100\n",
    "\n",
    "# Print metrics as percentages\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n",
    "print(f'Precision: {precision:.2f}%')\n",
    "print(f'Recall: {recall:.2f}%')\n",
    "print(f'F1 Score: {f1:.2f}%')\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_labels, y_pred, average=None)\n",
    "\n",
    "# Convert to percentages\n",
    "precision = precision * 100\n",
    "recall = recall * 100\n",
    "f1 = f1 * 100\n",
    "\n",
    "# Print formatted classification report\n",
    "print(\"\\nClassification Report (Percentage):\")\n",
    "print(f\"{'Class':<10} {'Precision':<10} {'Recall':<10} {'F1 Score':<10}\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"{class_name:<10} {precision[i]:<10.2f}% {recall[i]:<10.2f}% {f1[i]:<10.2f}%\")\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_labels, y_pred))\n",
    "\n",
    "# Preprocess nutritional data for meal recommendations\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a mapping function to map risk categories to meal clusters\n",
    "def map_risk_to_cluster(risk_category):\n",
    "    if risk_category == 'Low':\n",
    "        return 0  # Low-risk cluster\n",
    "    elif risk_category == 'Medium':\n",
    "        return 1  # Medium-risk cluster\n",
    "    else:\n",
    "        return 2  # High-risk cluster\n",
    "\n",
    "# Function to display meal clusters in a user-friendly format\n",
    "def display_meal_cluster(meals, cluster_name, num_meals=5):\n",
    "    print(f\"\\n=== Recommended Meals for {cluster_name} Cluster ===\")\n",
    "    print(f\"Total Meals in Cluster: {len(meals)}\")\n",
    "    \n",
    "    # Display a subset of meals\n",
    "    print(f\"\\nTop {num_meals} Meals:\")\n",
    "    print(f\"{'Meal Description':<50} {'Energy (kcal)':<15} {'Protein (g)':<15} {'Fat (g)':<15} {'Carbs (g)':<15}\")\n",
    "    print(\"-\" * 110)\n",
    "    for _, meal in meals.head(num_meals).iterrows():\n",
    "        print(f\"{meal['Descrip']:<50} {meal['Energy_kcal']:<15} {meal['Protein_g']:<15} {meal['Fat_g']:<15} {meal['Carb_g']:<15}\")\n",
    "    \n",
    "    # Display summary of nutritional values\n",
    "    avg_energy = meals['Energy_kcal'].mean()\n",
    "    avg_protein = meals['Protein_g'].mean()\n",
    "    avg_fat = meals['Fat_g'].mean()\n",
    "    avg_carbs = meals['Carb_g'].mean()\n",
    "    \n",
    "    print(\"\\n=== Nutritional Summary ===\")\n",
    "    print(f\"Average Energy (kcal): {avg_energy:.2f}\")\n",
    "    print(f\"Average Protein (g): {avg_protein:.2f}\")\n",
    "    print(f\"Average Fat (g): {avg_fat:.2f}\")\n",
    "    print(f\"Average Carbs (g): {avg_carbs:.2f}\")\n",
    "\n",
    "# Map cluster numbers to meaningful names\n",
    "cluster_names = {\n",
    "    0: \"Low-Risk\",\n",
    "    1: \"Medium-Risk\",\n",
    "    2: \"High-Risk\"\n",
    "}\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = pd.DataFrame({\n",
    "    'BMI': [24.5],\n",
    "    'Physical_Activity': [3],  # Assuming 3 is moderate activity\n",
    "    'Diet_Type_High-Carb': [0],\n",
    "    'Diet_Type_High-Fat': [1],\n",
    "    'MC4R_Variant_rs17782313_CT': [0],\n",
    "    'MC4R_Variant_rs17782313_TT': [1],\n",
    "    'PPARG_Variant_rs1801282_CG': [0],\n",
    "    'PPARG_Variant_rs1801282_GG': [0],\n",
    "    'FTO_Variant_rs9939609_AT': [0],\n",
    "    'FTO_Variant_rs9939609_TT': [1],\n",
    "    'LEPR_Variant_rs1137101_AG': [1],\n",
    "    'LEPR_Variant_rs1137101_GG': [0],\n",
    "    'Obesity_Risk_Score': [0.55]\n",
    "})\n",
    "\n",
    "# Ensure the new profile has the same columns as the training data\n",
    "missing_cols = set(X_train.columns) - set(new_profile.columns)\n",
    "for col in missing_cols:\n",
    "    new_profile[col] = 0\n",
    "new_profile = new_profile[X_train.columns]\n",
    "\n",
    "# Predict risk category for the new profile\n",
    "predicted_risk_category = best_rf.predict(new_profile)\n",
    "print(f'Predicted Obesity Risk Category: {predicted_risk_category[0]}')\n",
    "\n",
    "# Map predicted risk category to a meal cluster\n",
    "recommended_cluster = map_risk_to_cluster(predicted_risk_category[0])\n",
    "recommended_cluster_name = cluster_names.get(recommended_cluster, \"Unknown\")\n",
    "\n",
    "# Recommend meals from the recommended cluster\n",
    "recommended_meals = meal_df[meal_df['Meal_Cluster'] == recommended_cluster]\n",
    "\n",
    "# Display recommended meals in a user-friendly format\n",
    "display_meal_cluster(recommended_meals, recommended_cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a5cacf9-2771-4d6a-b7a6-3149489a7b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(best_rf, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35fd6ed7-176d-4ebf-8276-9fa2703272e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Cross-Validation Accuracy (Noisy Data): 82.98%\n"
     ]
    }
   ],
   "source": [
    "# Updated Hyperparameter Grid with Regularization\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],  # Limit tree depth\n",
    "    'min_samples_split': [10, 20],  # Increase minimum samples to split\n",
    "    'min_samples_leaf': [2, 4],  # Increase minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on noisy data\n",
    "X_noisy = X_resampled + np.random.normal(0, 0.1, X_resampled.shape)\n",
    "cv_scores_noisy = cross_val_score(best_rf, X_noisy, y_resampled, cv=5, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy (Noisy Data): {cv_scores_noisy.mean() * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea956689-d199-46b7-a7f0-f2ba5228a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Cross-Validation Accuracy (Noisy Data): 81.25%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb_scores = cross_val_score(xgb, X_noisy, y_resampled, cv=5, scoring='accuracy')\n",
    "print(f'XGBoost Cross-Validation Accuracy (Noisy Data): {xgb_scores.mean() * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cedfac2-89ee-4e35-b27b-98a4d9140e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Cross-Validation Accuracy (Noisy Data): 41.55%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "svm_scores = cross_val_score(svm, X_noisy, y_resampled, cv=5, scoring='accuracy')\n",
    "print(f'SVM Cross-Validation Accuracy (Noisy Data): {svm_scores.mean() * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22f55d05-a8f3-4744-bada-3a73873aae1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[0;32m      4\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m300\u001b[39m),\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m15\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m     }\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    return cross_val_score(model, X_noisy, y_resampled, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "print(f'Best Hyperparameters: {study.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdd39e21-b99c-46db-8b8f-82947d7ef039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Validation Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n",
      "Cross-Validation Accuracy (Noisy Data): 82.08%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      1.00      1.00       182\n",
      "         Low       1.00      1.00      1.00       154\n",
      "      Medium       1.00      1.00      1.00       173\n",
      "\n",
      "    accuracy                           1.00       509\n",
      "   macro avg       1.00      1.00      1.00       509\n",
      "weighted avg       1.00      1.00      1.00       509\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[182   0   0]\n",
      " [  0 154   0]\n",
      " [  0   0 173]]\n",
      "Predicted Obesity Risk Category: Medium\n",
      "\n",
      "=== Recommended Meals for Medium-Risk Cluster ===\n",
      "Total Meals in Cluster: 779\n",
      "\n",
      "Top 5 Meals:\n",
      "Meal Description                                   Energy (kcal)   Protein (g)     Fat (g)         Carbs (g)      \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Soy flour, full-fat, roasted                       441.0           34.8            21.86           33.67          \n",
      "Fast foods, english muffin, with cheese and sausage 338.0           13.28           20.67           25.28          \n",
      "Peanuts, virginia, oil-roasted, without salt       578.0           25.87           48.62           19.86          \n",
      "Cheesefurter, cheese smokie, pork, beef            328.0           14.1            29.0            1.51           \n",
      "Restaurant, family style, fried mozzarella sticks  325.0           14.75           18.33           25.14          \n",
      "\n",
      "=== Nutritional Summary ===\n",
      "Average Energy (kcal): 347.87\n",
      "Average Protein (g): 16.66\n",
      "Average Fat (g): 26.54\n",
      "Average Carbs (g): 11.62\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Select relevant features\n",
    "features = ['BMI', 'Physical_Activity', 'Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant', 'Obesity_Risk_Score']\n",
    "genetic_df = genetic_df[features + ['Obesity_Risk_Category']]\n",
    "\n",
    "# One-hot encode categorical features (Diet_Type and genetic variants)\n",
    "genetic_df = pd.get_dummies(genetic_df, columns=['Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant'], drop_first=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X = genetic_df.drop(['Obesity_Risk_Category'], axis=1)\n",
    "y = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Convert Obesity_Risk_Category to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Ensure all columns in X are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "        X[col] = X[col].fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y_encoded)\n",
    "\n",
    "# Split resampled data into training (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],  # Limit tree depth\n",
    "    'min_samples_split': [10, 20],  # Increase minimum samples to split\n",
    "    'min_samples_leaf': [2, 4],  # Increase minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred) * 100\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on noisy data\n",
    "X_noisy = X_resampled + np.random.normal(0, 0.1, X_resampled.shape)  # Add Gaussian noise\n",
    "cv_scores_noisy = cross_val_score(best_rf, X_noisy, y_resampled, cv=5, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy (Noisy Data): {cv_scores_noisy.mean() * 100:.2f}%')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# Preprocess nutritional data for meal recommendations\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a mapping function to map risk categories to meal clusters\n",
    "def map_risk_to_cluster(risk_category):\n",
    "    if risk_category == 'Low':\n",
    "        return 0  # Low-risk cluster\n",
    "    elif risk_category == 'Medium':\n",
    "        return 1  # Medium-risk cluster\n",
    "    else:\n",
    "        return 2  # High-risk cluster\n",
    "\n",
    "# Function to display meal clusters in a user-friendly format\n",
    "def display_meal_cluster(meals, cluster_name, num_meals=5):\n",
    "    print(f\"\\n=== Recommended Meals for {cluster_name} Cluster ===\")\n",
    "    print(f\"Total Meals in Cluster: {len(meals)}\")\n",
    "    \n",
    "    # Display a subset of meals\n",
    "    print(f\"\\nTop {num_meals} Meals:\")\n",
    "    print(f\"{'Meal Description':<50} {'Energy (kcal)':<15} {'Protein (g)':<15} {'Fat (g)':<15} {'Carbs (g)':<15}\")\n",
    "    print(\"-\" * 110)\n",
    "    for _, meal in meals.head(num_meals).iterrows():\n",
    "        print(f\"{meal['Descrip']:<50} {meal['Energy_kcal']:<15} {meal['Protein_g']:<15} {meal['Fat_g']:<15} {meal['Carb_g']:<15}\")\n",
    "    \n",
    "    # Display summary of nutritional values\n",
    "    avg_energy = meals['Energy_kcal'].mean()\n",
    "    avg_protein = meals['Protein_g'].mean()\n",
    "    avg_fat = meals['Fat_g'].mean()\n",
    "    avg_carbs = meals['Carb_g'].mean()\n",
    "    \n",
    "    print(\"\\n=== Nutritional Summary ===\")\n",
    "    print(f\"Average Energy (kcal): {avg_energy:.2f}\")\n",
    "    print(f\"Average Protein (g): {avg_protein:.2f}\")\n",
    "    print(f\"Average Fat (g): {avg_fat:.2f}\")\n",
    "    print(f\"Average Carbs (g): {avg_carbs:.2f}\")\n",
    "\n",
    "# Map cluster numbers to meaningful names\n",
    "cluster_names = {\n",
    "    0: \"Low-Risk\",\n",
    "    1: \"Medium-Risk\",\n",
    "    2: \"High-Risk\"\n",
    "}\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = {\n",
    "    'BMI': 28.5,  # Example value\n",
    "    'Physical_Activity': 2,  # Example value (1: Low, 2: Moderate, 3: High)\n",
    "    'Diet_Type': 'High-Fat',  # Example value\n",
    "    'MC4R_Variant': 'rs17782313_CT',  # Example value\n",
    "    'PPARG_Variant': 'rs1801282_CG',  # Example value\n",
    "    'FTO_Variant': 'rs9939609_AT',  # Example value\n",
    "    'LEPR_Variant': 'rs1137101_AG',  # Example value\n",
    "    'Obesity_Risk_Score': 0.45  # Example value\n",
    "}\n",
    "\n",
    "# Convert the new profile to a DataFrame\n",
    "new_profile_df = pd.DataFrame([new_profile])\n",
    "\n",
    "# One-hot encode categorical features (same as during training)\n",
    "new_profile_df = pd.get_dummies(new_profile_df, columns=['Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant'], drop_first=True)\n",
    "\n",
    "# Ensure the new profile has the same columns as the training data\n",
    "missing_cols = set(X_train.columns) - set(new_profile_df.columns)\n",
    "for col in missing_cols:\n",
    "    new_profile_df[col] = 0  # Add missing columns with default value 0\n",
    "\n",
    "# Reorder columns to match the training data\n",
    "new_profile_df = new_profile_df[X_train.columns]\n",
    "\n",
    "# Predict the obesity risk category for the new profile\n",
    "predicted_risk_category = best_rf.predict(new_profile_df)\n",
    "predicted_risk_category_label = label_encoder.inverse_transform(predicted_risk_category)[0]\n",
    "print(f'Predicted Obesity Risk Category: {predicted_risk_category_label}')\n",
    "\n",
    "# Map predicted risk category to a meal cluster\n",
    "recommended_cluster = map_risk_to_cluster(predicted_risk_category_label)\n",
    "recommended_cluster_name = cluster_names.get(recommended_cluster, \"Unknown\")\n",
    "\n",
    "# Recommend meals from the recommended cluster\n",
    "recommended_meals = meal_df[meal_df['Meal_Cluster'] == recommended_cluster]\n",
    "\n",
    "# Display recommended meals in a user-friendly format\n",
    "display_meal_cluster(recommended_meals, recommended_cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cd18d62-c07e-48e1-b4a1-51f205fabb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Obesity Risk Category: Medium\n",
      "\n",
      "=== Recommended Meals for Medium-Risk Cluster ===\n",
      "Total Meals in Cluster: 950\n",
      "\n",
      "Top 5 Meals:\n",
      "Meal Description                                   Energy (kcal)   Protein (g)     Fat (g)         Carbs (g)      \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Egg, whole, dried                                  605.0           48.37           43.04           1.53           \n",
      "Egg, whole, dried, stabilized, glucose reduced     615.0           48.17           43.95           2.38           \n",
      "Tofu, dried-frozen (koyadofu)                      480.0           47.94           30.34           14.56          \n",
      "Tofu, dried-frozen (koyadofu), prepared with calcium sulfate 472.0           47.94           30.34           12.79          \n",
      "Cheese, parmesan, low sodium                       451.0           41.6            29.99           3.7            \n",
      "\n",
      "=== Nutritional Summary ===\n",
      "Average Energy (kcal): 426.27\n",
      "Average Protein (g): 14.14\n",
      "Average Fat (g): 37.32\n",
      "Average Carbs (g): 9.88\n"
     ]
    }
   ],
   "source": [
    "# Function to recommend meals dynamically based on user profile\n",
    "def recommend_meals(user_profile, meal_df, model, label_encoder, cluster_names, num_meals=5):\n",
    "    # Convert the user profile to a DataFrame\n",
    "    user_profile_df = pd.DataFrame([user_profile])\n",
    "    \n",
    "    # One-hot encode categorical features (same as during training)\n",
    "    user_profile_df = pd.get_dummies(user_profile_df, columns=['Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant'], drop_first=True)\n",
    "    \n",
    "    # Ensure the user profile has the same columns as the training data\n",
    "    missing_cols = set(X_train.columns) - set(user_profile_df.columns)\n",
    "    for col in missing_cols:\n",
    "        user_profile_df[col] = 0  # Add missing columns with default value 0\n",
    "    \n",
    "    # Reorder columns to match the training data\n",
    "    user_profile_df = user_profile_df[X_train.columns]\n",
    "    \n",
    "    # Predict the obesity risk category for the user profile\n",
    "    predicted_risk_category = model.predict(user_profile_df)\n",
    "    predicted_risk_category_label = label_encoder.inverse_transform(predicted_risk_category)[0]\n",
    "    print(f'Predicted Obesity Risk Category: {predicted_risk_category_label}')\n",
    "    \n",
    "    # Map predicted risk category to multiple clusters (dynamic mapping)\n",
    "    if predicted_risk_category_label == 'Low':\n",
    "        clusters = [0, 1]  # Example: Low-risk users get meals from clusters 0 and 1\n",
    "    elif predicted_risk_category_label == 'Medium':\n",
    "        clusters = [1, 2]  # Example: Medium-risk users get meals from clusters 1 and 2\n",
    "    else:\n",
    "        clusters = [2, 3]  # Example: High-risk users get meals from clusters 2 and 3\n",
    "    \n",
    "    # Recommend meals from the selected clusters\n",
    "    recommended_meals = meal_df[meal_df['Meal_Cluster'].isin(clusters)]\n",
    "    \n",
    "    # Sort meals by nutritional suitability (e.g., lower calories for high-risk users)\n",
    "    if predicted_risk_category_label == 'High':\n",
    "        recommended_meals = recommended_meals.sort_values(by='Energy_kcal', ascending=True)\n",
    "    else:\n",
    "        recommended_meals = recommended_meals.sort_values(by='Protein_g', ascending=False)\n",
    "    \n",
    "    # Display recommended meals in a user-friendly format\n",
    "    display_meal_cluster(recommended_meals, f\"{predicted_risk_category_label}-Risk\", num_meals)\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = {\n",
    "    'BMI': 28.5,  # Example value\n",
    "    'Physical_Activity': 2,  # Example value (1: Low, 2: Moderate, 3: High)\n",
    "    'Diet_Type': 'High-Fat',  # Example value\n",
    "    'MC4R_Variant': 'rs17782313_CT',  # Example value\n",
    "    'PPARG_Variant': 'rs1801282_CG',  # Example value\n",
    "    'FTO_Variant': 'rs9939609_AT',  # Example value\n",
    "    'LEPR_Variant': 'rs1137101_AG',  # Example value\n",
    "    'Obesity_Risk_Score': 0.45  # Example value\n",
    "}\n",
    "\n",
    "# Recommend meals for the new profile\n",
    "recommend_meals(new_profile, meal_df, best_rf, label_encoder, cluster_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0c2f16e-5a3a-4969-9c96-78e84b88152c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Validation Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n",
      "Cross-Validation Accuracy (Noisy Data): 82.86%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      1.00      1.00       182\n",
      "         Low       1.00      1.00      1.00       154\n",
      "      Medium       1.00      1.00      1.00       173\n",
      "\n",
      "    accuracy                           1.00       509\n",
      "   macro avg       1.00      1.00      1.00       509\n",
      "weighted avg       1.00      1.00      1.00       509\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[182   0   0]\n",
      " [  0 154   0]\n",
      " [  0   0 173]]\n",
      "Predicted Obesity Risk Category: Medium\n",
      "\n",
      "=== Recommended Meals for Medium-Risk ===\n",
      "Total Meals in Cluster: 3637\n",
      "\n",
      "Top 5 Meals:\n",
      "Meal Description                                   Energy (kcal)   Protein (g)     Fat (g)         Carbs (g)      \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Snacks, popcorn, oil-popped, microwave, regular flavor, no trans fat 583.0           7.29            43.55           45.06          \n",
      "Chocolate, dark, 60-69% cacao solids               579.0           6.12            38.31           52.42          \n",
      "Candies, HERSHEY'S POT OF GOLD Almond Bar          577.0           12.82           38.46           46.15          \n",
      "Candies, MARS SNACKFOOD US, COCOAVIA Chocolate Covered Almonds 573.0           9.51            37.07           50.22          \n",
      "Candies, HERSHEY'S MILK CHOCOLATE WITH ALMOND BITES 568.0           9.76            35.73           51.72          \n",
      "\n",
      "=== Nutritional Summary ===\n",
      "Average Energy (kcal): 146.01\n",
      "Average Protein (g): 7.73\n",
      "Average Fat (g): 5.88\n",
      "Average Carbs (g): 15.77\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Select relevant features\n",
    "features = ['BMI', 'Physical_Activity', 'Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant', 'Obesity_Risk_Score']\n",
    "genetic_df = genetic_df[features + ['Obesity_Risk_Category']]\n",
    "\n",
    "# One-hot encode categorical features (Diet_Type and genetic variants)\n",
    "genetic_df = pd.get_dummies(genetic_df, columns=['Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant'], drop_first=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X = genetic_df.drop(['Obesity_Risk_Category'], axis=1)\n",
    "y = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Convert Obesity_Risk_Category to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Ensure all columns in X are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "        X[col] = X[col].fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y_encoded)\n",
    "\n",
    "# Split resampled data into training (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],  # Limit tree depth\n",
    "    'min_samples_split': [10, 20],  # Increase minimum samples to split\n",
    "    'min_samples_leaf': [2, 4],  # Increase minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred) * 100\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on noisy data\n",
    "X_noisy = X_resampled + np.random.normal(0, 0.1, X_resampled.shape)  # Add Gaussian noise\n",
    "cv_scores_noisy = cross_val_score(best_rf, X_noisy, y_resampled, cv=5, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy (Noisy Data): {cv_scores_noisy.mean() * 100:.2f}%')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# Preprocess nutritional data for meal recommendations\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Increase the number of meal clusters\n",
    "num_clusters = 10  # Use 10 clusters for more diversity\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Function to display meal clusters in a user-friendly format\n",
    "def display_meal_cluster(meals, cluster_name, num_meals=5):\n",
    "    print(f\"\\n=== Recommended Meals for {cluster_name} ===\")\n",
    "    print(f\"Total Meals in Cluster: {len(meals)}\")\n",
    "    \n",
    "    # Display a subset of meals\n",
    "    print(f\"\\nTop {num_meals} Meals:\")\n",
    "    print(f\"{'Meal Description':<50} {'Energy (kcal)':<15} {'Protein (g)':<15} {'Fat (g)':<15} {'Carbs (g)':<15}\")\n",
    "    print(\"-\" * 110)\n",
    "    for _, meal in meals.head(num_meals).iterrows():\n",
    "        print(f\"{meal['Descrip']:<50} {meal['Energy_kcal']:<15} {meal['Protein_g']:<15} {meal['Fat_g']:<15} {meal['Carb_g']:<15}\")\n",
    "    \n",
    "    # Display summary of nutritional values\n",
    "    avg_energy = meals['Energy_kcal'].mean()\n",
    "    avg_protein = meals['Protein_g'].mean()\n",
    "    avg_fat = meals['Fat_g'].mean()\n",
    "    avg_carbs = meals['Carb_g'].mean()\n",
    "    \n",
    "    print(\"\\n=== Nutritional Summary ===\")\n",
    "    print(f\"Average Energy (kcal): {avg_energy:.2f}\")\n",
    "    print(f\"Average Protein (g): {avg_protein:.2f}\")\n",
    "    print(f\"Average Fat (g): {avg_fat:.2f}\")\n",
    "    print(f\"Average Carbs (g): {avg_carbs:.2f}\")\n",
    "\n",
    "# Function to recommend meals dynamically based on user profile\n",
    "def recommend_meals(user_profile, meal_df, model, label_encoder, num_meals=5):\n",
    "    # Convert the user profile to a DataFrame\n",
    "    user_profile_df = pd.DataFrame([user_profile])\n",
    "    \n",
    "    # One-hot encode categorical features (same as during training)\n",
    "    user_profile_df = pd.get_dummies(user_profile_df, columns=['Diet_Type', 'MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant'], drop_first=True)\n",
    "    \n",
    "    # Ensure the user profile has the same columns as the training data\n",
    "    missing_cols = set(X_train.columns) - set(user_profile_df.columns)\n",
    "    for col in missing_cols:\n",
    "        user_profile_df[col] = 0  # Add missing columns with default value 0\n",
    "    \n",
    "    # Reorder columns to match the training data\n",
    "    user_profile_df = user_profile_df[X_train.columns]\n",
    "    \n",
    "    # Predict the obesity risk category for the user profile\n",
    "    predicted_risk_category = model.predict(user_profile_df)\n",
    "    predicted_risk_category_label = label_encoder.inverse_transform(predicted_risk_category)[0]\n",
    "    print(f'Predicted Obesity Risk Category: {predicted_risk_category_label}')\n",
    "    \n",
    "    # Define cluster preferences based on risk category\n",
    "    if predicted_risk_category_label == 'Low':\n",
    "        # Low-risk users: Focus on high-protein, balanced meals\n",
    "        preferred_clusters = [0, 1, 2]  # Example clusters\n",
    "        sort_by = 'Protein_g'  # Sort by highest protein\n",
    "    elif predicted_risk_category_label == 'Medium':\n",
    "        # Medium-risk users: Focus on moderate-calorie, balanced meals\n",
    "        preferred_clusters = [3, 4, 5]  # Example clusters\n",
    "        sort_by = 'Energy_kcal'  # Sort by moderate calories\n",
    "    else:\n",
    "        # High-risk users: Focus on low-calorie, nutrient-dense meals\n",
    "        preferred_clusters = [6, 7, 8, 9]  # Example clusters\n",
    "        sort_by = 'Energy_kcal'  # Sort by lowest calories\n",
    "    \n",
    "    # Recommend meals from the preferred clusters\n",
    "    recommended_meals = meal_df[meal_df['Meal_Cluster'].isin(preferred_clusters)]\n",
    "    \n",
    "    # Sort meals based on the user's risk category\n",
    "    if predicted_risk_category_label == 'High':\n",
    "        recommended_meals = recommended_meals.sort_values(by=sort_by, ascending=True)  # Low calories\n",
    "    else:\n",
    "        recommended_meals = recommended_meals.sort_values(by=sort_by, ascending=False)  # High protein or moderate calories\n",
    "    \n",
    "    # Display recommended meals in a user-friendly format\n",
    "    display_meal_cluster(recommended_meals, f\"{predicted_risk_category_label}-Risk\", num_meals)\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = {\n",
    "    'BMI': 28.5,  # Example value\n",
    "    'Physical_Activity': 2,  # Example value (1: Low, 2: Moderate, 3: High)\n",
    "    'Diet_Type': 'High-Fat',  # Example value\n",
    "    'MC4R_Variant': 'rs17782313_CT',  # Example value\n",
    "    'PPARG_Variant': 'rs1801282_CG',  # Example value\n",
    "    'FTO_Variant': 'rs9939609_AT',  # Example value\n",
    "    'LEPR_Variant': 'rs1137101_AG',  # Example value\n",
    "    'Obesity_Risk_Score': 0.45  # Example value\n",
    "}\n",
    "\n",
    "# Recommend meals for the new profile\n",
    "recommend_meals(new_profile, meal_df, best_rf, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982791b1-9da7-48b6-b6c5-5c5eccddf41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Accuracy: 0.3550\n",
      "Precision: 0.3552\n",
      "Recall: 0.3550\n",
      "F1 Score: 0.3547\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.36      0.36       144\n",
      "           1       0.35      0.38      0.36       130\n",
      "           2       0.35      0.33      0.34       126\n",
      "\n",
      "    accuracy                           0.35       400\n",
      "   macro avg       0.35      0.35      0.35       400\n",
      "weighted avg       0.36      0.35      0.35       400\n",
      "\n",
      "\n",
      "Sample Predictions:\n",
      "   Actual  Predicted\n",
      "0       0          2\n",
      "1       0          2\n",
      "2       0          1\n",
      "3       1          2\n",
      "4       0          1\n",
      "5       2          1\n",
      "6       0          1\n",
      "7       0          1\n",
      "8       2          0\n",
      "9       1          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "genetic_df = pd.read_csv('C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\genetic_profiles.csv')\n",
    "\n",
    "# One-Hot Encoding for categorical variables\n",
    "categorical_cols = ['MC4R_Variant', 'PPARG_Variant', 'FTO_Variant', 'LEPR_Variant', 'Physical_Activity']\n",
    "genetic_df = pd.get_dummies(genetic_df, columns=categorical_cols)\n",
    "\n",
    "# Separate features and target\n",
    "X = genetic_df.drop(columns=['Profile_ID', 'Diet_Type'])  # Removing Profile_ID & target\n",
    "\n",
    "y = genetic_df['Diet_Type'].astype('category').cat.codes  # Convert target to numerical labels\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X[['Age', 'BMI']] = scaler.fit_transform(X[['Age', 'BMI']])\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model with class weights\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display some sample predictions\n",
    "sample_preds = pd.DataFrame({'Actual': y_test[:10].values, 'Predicted': y_pred[:10]})\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(sample_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aa71cf-222b-4f0e-a4d0-e3e024b96f3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Apply SMOTE to balance the dataset\u001b[39;00m\n\u001b[0;32m     78\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Split resampled data into training (60%), validation (20%), and test (20%) sets\u001b[39;00m\n\u001b[0;32m     82\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(X_resampled, y_resampled, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\base.py:99\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m     97\u001b[0m check_classification_targets(y)\n\u001b[0;32m     98\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m---> 99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\base.py:157\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    155\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    156\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 157\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\new_genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Map gene variants to numerical values based on their functional impact\n",
    "variant_mapping = {\n",
    "    'MC4R_Variant': {'rs17782313_CT': 0.5, 'rs17782313_TT': 1.0, 'rs17782313_CC': 0.0, 'None': 0},\n",
    "    'PPARG_Variant': {'rs1801282_CG': 0.3, 'rs1801282_GG': 0.7, 'rs1801282_CC': 0.0, 'None': 0},\n",
    "    'FTO_Variant': {'rs9939609_AT': 0.4, 'rs9939609_TT': 0.8, 'rs9939609_AA': 0.0, 'None': 0},\n",
    "    'LEPR_Variant': {'rs1137101_AG': 0.6, 'rs1137101_GG': 0.9, 'rs1137101_AA': 0.0, 'None': 0}\n",
    "}\n",
    "\n",
    "for variant, mapping in variant_mapping.items():\n",
    "    genetic_df[variant] = genetic_df[variant].map(mapping)\n",
    "\n",
    "# Create a genetic risk score\n",
    "genetic_df['Genetic_Risk_Score'] = (\n",
    "    genetic_df['MC4R_Present'] * genetic_df['MC4R_Variant'] +\n",
    "    genetic_df['PPARG_Present'] * genetic_df['PPARG_Variant'] +\n",
    "    genetic_df['FTO_Present'] * genetic_df['FTO_Variant'] +\n",
    "    genetic_df['LEPR_Present'] * genetic_df['LEPR_Variant']\n",
    ")\n",
    "\n",
    "# Create interaction features\n",
    "genetic_df['BMI_MC4R_Interaction'] = genetic_df['BMI'] * genetic_df['MC4R_Present']\n",
    "genetic_df['Physical_Activity_FTO_Interaction'] = genetic_df['Physical_Activity'] * genetic_df['FTO_Present']\n",
    "\n",
    "# Select relevant features\n",
    "features = [\n",
    "    'BMI', 'Physical_Activity', 'Diet_Type', 'Obesity_Risk_Score',\n",
    "    'MC4R_Present', 'PPARG_Present', 'FTO_Present', 'LEPR_Present',\n",
    "    'Genetic_Risk_Score', 'BMI_MC4R_Interaction', 'Physical_Activity_FTO_Interaction'\n",
    "]\n",
    "genetic_df = genetic_df[features + ['Obesity_Risk_Category']]\n",
    "\n",
    "# One-hot encode only Diet_Type (not genetic variants)\n",
    "genetic_df = pd.get_dummies(genetic_df, columns=['Diet_Type'], drop_first=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X = genetic_df.drop(['Obesity_Risk_Category'], axis=1)\n",
    "y = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Convert Obesity_Risk_Category to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Ensure all columns in X are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "        X[col] = X[col].fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y_encoded)\n",
    "\n",
    "# Split resampled data into training (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],  # Limit tree depth\n",
    "    'min_samples_split': [10, 20],  # Increase minimum samples to split\n",
    "    'min_samples_leaf': [2, 4],  # Increase minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred) * 100\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on noisy data\n",
    "X_noisy = X_resampled + np.random.normal(0, 0.1, X_resampled.shape)  # Add Gaussian noise\n",
    "cv_scores_noisy = cross_val_score(best_rf, X_noisy, y_resampled, cv=5, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy (Noisy Data): {cv_scores_noisy.mean() * 100:.2f}%')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# Preprocess nutritional data for meal recommendations\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Increase the number of meal clusters\n",
    "num_clusters = 10  # Use 10 clusters for more diversity\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Function to display meal clusters in a user-friendly format\n",
    "def display_meal_cluster(meals, cluster_name, num_meals=5):\n",
    "    print(f\"\\n=== Recommended Meals for {cluster_name} ===\")\n",
    "    print(f\"Total Meals in Cluster: {len(meals)}\")\n",
    "    \n",
    "    # Display a subset of meals\n",
    "    print(f\"\\nTop {num_meals} Meals:\")\n",
    "    print(f\"{'Meal Description':<50} {'Energy (kcal)':<15} {'Protein (g)':<15} {'Fat (g)':<15} {'Carbs (g)':<15}\")\n",
    "    print(\"-\" * 110)\n",
    "    for _, meal in meals.head(num_meals).iterrows():\n",
    "        print(f\"{meal['Descrip']:<50} {meal['Energy_kcal']:<15} {meal['Protein_g']:<15} {meal['Fat_g']:<15} {meal['Carb_g']:<15}\")\n",
    "    \n",
    "    # Display summary of nutritional values\n",
    "    avg_energy = meals['Energy_kcal'].mean()\n",
    "    avg_protein = meals['Protein_g'].mean()\n",
    "    avg_fat = meals['Fat_g'].mean()\n",
    "    avg_carbs = meals['Carb_g'].mean()\n",
    "    \n",
    "    print(\"\\n=== Nutritional Summary ===\")\n",
    "    print(f\"Average Energy (kcal): {avg_energy:.2f}\")\n",
    "    print(f\"Average Protein (g): {avg_protein:.2f}\")\n",
    "    print(f\"Average Fat (g): {avg_fat:.2f}\")\n",
    "    print(f\"Average Carbs (g): {avg_carbs:.2f}\")\n",
    "\n",
    "# Function to recommend meals dynamically based on user profile\n",
    "def recommend_meals(user_profile, meal_df, model, label_encoder, num_meals=5):\n",
    "    # Convert the user profile to a DataFrame\n",
    "    user_profile_df = pd.DataFrame([user_profile])\n",
    "    \n",
    "    # One-hot encode categorical features (same as during training)\n",
    "    user_profile_df = pd.get_dummies(user_profile_df, columns=['Diet_Type'], drop_first=True)\n",
    "    \n",
    "    # Ensure the user profile has the same columns as the training data\n",
    "    missing_cols = set(X_train.columns) - set(user_profile_df.columns)\n",
    "    for col in missing_cols:\n",
    "        user_profile_df[col] = 0  # Add missing columns with default value 0\n",
    "    \n",
    "    # Reorder columns to match the training data\n",
    "    user_profile_df = user_profile_df[X_train.columns]\n",
    "    \n",
    "    # Predict the obesity risk category for the user profile\n",
    "    predicted_risk_category = model.predict(user_profile_df)\n",
    "    predicted_risk_category_label = label_encoder.inverse_transform(predicted_risk_category)[0]\n",
    "    print(f'Predicted Obesity Risk Category: {predicted_risk_category_label}')\n",
    "    \n",
    "    # Define cluster preferences based on risk category\n",
    "    if predicted_risk_category_label == 'Low':\n",
    "        # Low-risk users: Focus on high-protein, balanced meals\n",
    "        preferred_clusters = [0, 1, 2]  # Example clusters\n",
    "        sort_by = 'Protein_g'  # Sort by highest protein\n",
    "    elif predicted_risk_category_label == 'Medium':\n",
    "        # Medium-risk users: Focus on moderate-calorie, balanced meals\n",
    "        preferred_clusters = [3, 4, 5]  # Example clusters\n",
    "        sort_by = 'Energy_kcal'  # Sort by moderate calories\n",
    "    else:\n",
    "        # High-risk users: Focus on low-calorie, nutrient-dense meals\n",
    "        preferred_clusters = [6, 7, 8, 9]  # Example clusters\n",
    "        sort_by = 'Energy_kcal'  # Sort by lowest calories\n",
    "    \n",
    "    # Recommend meals from the preferred clusters\n",
    "    recommended_meals = meal_df[meal_df['Meal_Cluster'].isin(preferred_clusters)]\n",
    "    \n",
    "    # Sort meals based on the user's risk category\n",
    "    if predicted_risk_category_label == 'High':\n",
    "        recommended_meals = recommended_meals.sort_values(by=sort_by, ascending=True)  # Low calories\n",
    "    else:\n",
    "        recommended_meals = recommended_meals.sort_values(by=sort_by, ascending=False)  # High protein or moderate calories\n",
    "    \n",
    "    # Display recommended meals in a user-friendly format\n",
    "    display_meal_cluster(recommended_meals, f\"{predicted_risk_category_label}-Risk\", num_meals)\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = {\n",
    "    'BMI': 28.5,  # Example value\n",
    "    'Physical_Activity': 2,  # Example value (1: Low, 2: Moderate, 3: High)\n",
    "    'Diet_Type': 'High-Fat',  # Example value\n",
    "    'Obesity_Risk_Score': 0.45,  # Example value\n",
    "    'MC4R_Present': 1,  # Example value\n",
    "    'PPARG_Present': 0,  # Example value\n",
    "    'FTO_Present': 1,  # Example value\n",
    "    'LEPR_Present': 1  # Example value\n",
    "}\n",
    "\n",
    "# Recommend meals for the new profile\n",
    "recommend_meals(new_profile, meal_df, best_rf, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af7fdb8-1929-462a-92ee-aa7d57b9e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMI                                     0\n",
      "Physical_Activity                       0\n",
      "Obesity_Risk_Score                      0\n",
      "MC4R_Present                            0\n",
      "PPARG_Present                           0\n",
      "FTO_Present                             0\n",
      "LEPR_Present                            0\n",
      "Genetic_Risk_Score                   1469\n",
      "BMI_MC4R_Interaction                    0\n",
      "Physical_Activity_FTO_Interaction       0\n",
      "Diet_Type_High-Carb                     0\n",
      "Diet_Type_High-Fat                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1c0d184-6760-422a-ba2c-9f9d054ef29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMI                                     0\n",
      "Physical_Activity                       0\n",
      "Obesity_Risk_Score                      0\n",
      "MC4R_Present                            0\n",
      "PPARG_Present                           0\n",
      "FTO_Present                             0\n",
      "LEPR_Present                            0\n",
      "Genetic_Risk_Score                   1469\n",
      "BMI_MC4R_Interaction                    0\n",
      "Physical_Activity_FTO_Interaction       0\n",
      "Diet_Type_High-Carb                     0\n",
      "Diet_Type_High-Fat                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feed35b8-cf3e-430e-84d0-b97cd8c70a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Validation Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n",
      "Cross-Validation Accuracy (Noisy Data): 87.03%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      1.00      1.00       194\n",
      "         Low       1.00      1.00      1.00       202\n",
      "      Medium       1.00      1.00      1.00       167\n",
      "\n",
      "    accuracy                           1.00       563\n",
      "   macro avg       1.00      1.00      1.00       563\n",
      "weighted avg       1.00      1.00      1.00       563\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[194   0   0]\n",
      " [  0 202   0]\n",
      " [  0   0 167]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1036, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1548, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Obesity Risk Category: High\n",
      "\n",
      "=== Recommended Meals for High-Risk ===\n",
      "Total Meals in Cluster: 1941\n",
      "\n",
      "Top 5 Meals:\n",
      "Meal Description                                   Energy (kcal)   Protein (g)     Fat (g)         Carbs (g)      \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Sweetener, herbal extract powder from Stevia leaf  0.0             0.0             0.0             100.0          \n",
      "Turkey, breast, from whole bird, enhanced, meat only, roasted 127.0           26.97           2.08            0.0            \n",
      "Turkey from whole, enhanced, light meat, meat only, cooked, roasted 127.0           26.97           2.08            0.0            \n",
      "Turkey, wing, from whole bird, enhanced, meat only, roasted 127.0           26.97           2.08            0.0            \n",
      "Game meat, buffalo, water, cooked, roasted         131.0           26.83           1.8             0.0            \n",
      "\n",
      "=== Nutritional Summary ===\n",
      "Average Energy (kcal): 315.06\n",
      "Average Protein (g): 18.80\n",
      "Average Fat (g): 11.42\n",
      "Average Carbs (g): 36.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "genetic_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\new_genetic_profiles.csv\")\n",
    "meal_df = pd.read_csv(\"C:\\\\Users\\\\trejan\\\\Desktop\\\\Sem 2\\\\Machine Learning\\\\model\\\\train.csv\")\n",
    "\n",
    "# Handle missing values for numeric columns in genetic_df\n",
    "numeric_cols = genetic_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "genetic_df[numeric_cols] = genetic_df[numeric_cols].fillna(genetic_df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for categorical columns in genetic_df\n",
    "categorical_cols = genetic_df.select_dtypes(include=['object']).columns\n",
    "genetic_df[categorical_cols] = genetic_df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (low, medium, high)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Calculate Genetic_Risk_Score using only presence/absence columns\n",
    "genetic_df['Genetic_Risk_Score'] = (\n",
    "    genetic_df['MC4R_Present'] +\n",
    "    genetic_df['PPARG_Present'] +\n",
    "    genetic_df['FTO_Present'] +\n",
    "    genetic_df['LEPR_Present']\n",
    ")\n",
    "\n",
    "# Create interaction features\n",
    "genetic_df['BMI_MC4R_Interaction'] = genetic_df['BMI'] * genetic_df['MC4R_Present']\n",
    "genetic_df['Physical_Activity_FTO_Interaction'] = genetic_df['Physical_Activity'] * genetic_df['FTO_Present']\n",
    "\n",
    "# Select relevant features\n",
    "features = [\n",
    "    'BMI', 'Physical_Activity', 'Diet_Type', 'Obesity_Risk_Score',\n",
    "    'MC4R_Present', 'PPARG_Present', 'FTO_Present', 'LEPR_Present',\n",
    "    'Genetic_Risk_Score', 'BMI_MC4R_Interaction', 'Physical_Activity_FTO_Interaction'\n",
    "]\n",
    "genetic_df = genetic_df[features + ['Obesity_Risk_Category']]\n",
    "\n",
    "# One-hot encode only Diet_Type (not genetic variants)\n",
    "genetic_df = pd.get_dummies(genetic_df, columns=['Diet_Type'], drop_first=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X = genetic_df.drop(['Obesity_Risk_Category'], axis=1)\n",
    "y = genetic_df['Obesity_Risk_Category']\n",
    "\n",
    "# Convert Obesity_Risk_Category to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Ensure all columns in X are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "        X[col] = X[col].fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y_encoded)\n",
    "\n",
    "# Split resampled data into training (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],  # Limit tree depth\n",
    "    'min_samples_split': [10, 20],  # Increase minimum samples to split\n",
    "    'min_samples_leaf': [2, 4],  # Increase minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Use RandomForestClassifier for classification\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred) * 100\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on noisy data\n",
    "X_noisy = X_resampled + np.random.normal(0, 0.1, X_resampled.shape)  # Add Gaussian noise\n",
    "cv_scores_noisy = cross_val_score(best_rf, X_noisy, y_resampled, cv=5, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy (Noisy Data): {cv_scores_noisy.mean() * 100:.2f}%')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# Preprocess nutritional data for meal recommendations\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Increase the number of meal clusters\n",
    "num_clusters = 10  # Use 10 clusters for more diversity\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Function to display meal clusters in a user-friendly format\n",
    "def display_meal_cluster(meals, cluster_name, num_meals=5):\n",
    "    print(f\"\\n=== Recommended Meals for {cluster_name} ===\")\n",
    "    print(f\"Total Meals in Cluster: {len(meals)}\")\n",
    "    \n",
    "    # Display a subset of meals\n",
    "    print(f\"\\nTop {num_meals} Meals:\")\n",
    "    print(f\"{'Meal Description':<50} {'Energy (kcal)':<15} {'Protein (g)':<15} {'Fat (g)':<15} {'Carbs (g)':<15}\")\n",
    "    print(\"-\" * 110)\n",
    "    for _, meal in meals.head(num_meals).iterrows():\n",
    "        print(f\"{meal['Descrip']:<50} {meal['Energy_kcal']:<15} {meal['Protein_g']:<15} {meal['Fat_g']:<15} {meal['Carb_g']:<15}\")\n",
    "    \n",
    "    # Display summary of nutritional values\n",
    "    avg_energy = meals['Energy_kcal'].mean()\n",
    "    avg_protein = meals['Protein_g'].mean()\n",
    "    avg_fat = meals['Fat_g'].mean()\n",
    "    avg_carbs = meals['Carb_g'].mean()\n",
    "    \n",
    "    print(\"\\n=== Nutritional Summary ===\")\n",
    "    print(f\"Average Energy (kcal): {avg_energy:.2f}\")\n",
    "    print(f\"Average Protein (g): {avg_protein:.2f}\")\n",
    "    print(f\"Average Fat (g): {avg_fat:.2f}\")\n",
    "    print(f\"Average Carbs (g): {avg_carbs:.2f}\")\n",
    "\n",
    "# Function to recommend meals dynamically based on user profile\n",
    "def recommend_meals(user_profile, meal_df, model, label_encoder, num_meals=5):\n",
    "    # Convert the user profile to a DataFrame\n",
    "    user_profile_df = pd.DataFrame([user_profile])\n",
    "    \n",
    "    # One-hot encode categorical features (same as during training)\n",
    "    user_profile_df = pd.get_dummies(user_profile_df, columns=['Diet_Type'], drop_first=True)\n",
    "    \n",
    "    # Ensure the user profile has the same columns as the training data\n",
    "    missing_cols = set(X_train.columns) - set(user_profile_df.columns)\n",
    "    for col in missing_cols:\n",
    "        user_profile_df[col] = 0  # Add missing columns with default value 0\n",
    "    \n",
    "    # Reorder columns to match the training data\n",
    "    user_profile_df = user_profile_df[X_train.columns]\n",
    "    \n",
    "    # Predict the obesity risk category for the user profile\n",
    "    predicted_risk_category = model.predict(user_profile_df)\n",
    "    predicted_risk_category_label = label_encoder.inverse_transform(predicted_risk_category)[0]\n",
    "    print(f'Predicted Obesity Risk Category: {predicted_risk_category_label}')\n",
    "    \n",
    "    # Define cluster preferences based on risk category\n",
    "    if predicted_risk_category_label == 'Low':\n",
    "        # Low-risk users: Focus on high-protein, balanced meals\n",
    "        preferred_clusters = [0, 1, 2]  # Example clusters\n",
    "        sort_by = 'Protein_g'  # Sort by highest protein\n",
    "    elif predicted_risk_category_label == 'Medium':\n",
    "        # Medium-risk users: Focus on moderate-calorie, balanced meals\n",
    "        preferred_clusters = [3, 4, 5]  # Example clusters\n",
    "        sort_by = 'Energy_kcal'  # Sort by moderate calories\n",
    "    else:\n",
    "        # High-risk users: Focus on low-calorie, nutrient-dense meals\n",
    "        preferred_clusters = [6, 7, 8, 9]  # Example clusters\n",
    "        sort_by = 'Energy_kcal'  # Sort by lowest calories\n",
    "    \n",
    "    # Recommend meals from the preferred clusters\n",
    "    recommended_meals = meal_df[meal_df['Meal_Cluster'].isin(preferred_clusters)]\n",
    "    \n",
    "    # Sort meals based on the user's risk category\n",
    "    if predicted_risk_category_label == 'High':\n",
    "        recommended_meals = recommended_meals.sort_values(by=sort_by, ascending=True)  # Low calories\n",
    "    else:\n",
    "        recommended_meals = recommended_meals.sort_values(by=sort_by, ascending=False)  # High protein or moderate calories\n",
    "    \n",
    "    # Display recommended meals in a user-friendly format\n",
    "    display_meal_cluster(recommended_meals, f\"{predicted_risk_category_label}-Risk\", num_meals)\n",
    "\n",
    "# Example new genetic profile\n",
    "new_profile = {\n",
    "    'BMI': 28.5,  # Example value\n",
    "    'Physical_Activity': 3,  # Example value (1: Low, 2: Moderate, 3: High)\n",
    "    'Diet_Type': 'High-Fat',  # Example value\n",
    "    'Obesity_Risk_Score': 0.8,  # Example value\n",
    "    'MC4R_Present': 1,  # Example value\n",
    "    'PPARG_Present': 0,  # Example value\n",
    "    'FTO_Present': 1,  # Example value\n",
    "    'LEPR_Present': 0  # Example value\n",
    "}\n",
    "\n",
    "# Recommend meals for the new profile\n",
    "recommend_meals(new_profile, meal_df, best_rf, label_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
